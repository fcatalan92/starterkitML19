{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning starterkit\n",
    "Welcome to the tutorial! First of all we need to import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "import analysis_utils as au\n",
    "\n",
    "# avoid pandas warning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few infos on the libraries\n",
    "\n",
    "### uproot\n",
    "\n",
    "uproot is a Python package that provides tools for reading/writing ROOT files using Python and Numpy (does not depend on ROOT) and is primarly intended to stream data into machine learning libraries in Python.\n",
    "\n",
    "We use uproot for reading and converting ROOT Trees into ***pandas*** **DataFrame**.\n",
    "For more details: https://github.com/scikit-hep/uproot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load our data using uproot: signal and background for the training of the models and the unknown data on which we would like to have the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_tree = uproot.open('/eos/user/a/alicesk/sk2019/data/SignalTree.root')['SignalTable']\n",
    "background_tree = uproot.open('/eos/user/a/alicesk/sk2019/data/LikeSignTree.root')['BackgroundTable']\n",
    "\n",
    "data_tree = uproot.open('/eos/user/a/alicesk/sk2019/data/DataTree.root')['DataTable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_tree.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert our trees in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signal = signal_tree.pandas.df()\n",
    "df_background = background_tree.pandas.df()\n",
    "df_data = data_tree.pandas.df()\n",
    "\n",
    "del signal_tree\n",
    "del background_tree\n",
    "del data_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas \n",
    "\n",
    "Pandas is a library that provides data structures and analysis tools for Pyhton. The two primary data structures of pandas are **Series** (1-dimensional) and **DataFrame** (2-dimensional) and we will work with them.\n",
    "\n",
    "- **Series** are 1-dimensional ndarray with axis labels.\n",
    "- **DataFrame** are 2-dimensional tabular data structure with labeled axes (rows and columns).\n",
    "\n",
    "For more details: https://pandas.pydata.org/pandas-docs/stable/\n",
    "\n",
    "### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quark_list = ['Up', 'Down', 'Charm', 'Strange', 'Top', 'Bottom']\n",
    "quark_ser = pd.Series(quark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quark_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quark_ser.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quark_ser.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data and index must have same lenght (obviously)!\n",
    "quark_indices = ['q1', 'q2', 'q3', 'q4', 'q5', 'q6']\n",
    "# Series with custom indexing\n",
    "quark_ser = pd.Series(data=quark_list, index=quark_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can acces elements by element position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or using  index label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to do operations between series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series([1,2,3,4,5])\n",
    "ser2 = pd.Series([5,4,3,2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_sum.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_product.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "Can be thought of as a dict-like container for Series objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quark_df = pd.DataFrame(data=quark_list, columns=['names'])\n",
    "quark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add more columns to this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "symbols = ['u', 'd', 'c', 's', 't', 'b']\n",
    "charge = [2/3, -1/3, 2/3, -1/3, 2/3, -1/3]\n",
    "generation = [1, 1, 2, 2, 3, 3]\n",
    "\n",
    "quark_df['symbol'] = symbols\n",
    "quark_df['charge'] = charge\n",
    "quark_df['generation'] = generation\n",
    "\n",
    "quark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns in a pandas Dataframe can be accessed as dictionaries and return a pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create a Dataframe from a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaty with a list as value\n",
    "dictionary = {'integer': range(0,1000)}\n",
    "df = pd.DataFrame(dictionary)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make operations on the columns and store the result in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['pow2'] = df.integer * df.integer\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "One of the most interesting tool of DataFrame is the *query()* method (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html). With this method we can query the DataFrame getting elements which **satisfy a boolean expression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.query('integer > 100 and pow2 < 14000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get back to our data\n",
    "We can inspect our data easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_signal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also quick to make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#invariant-mass distribution of the signal\n",
    "minv_sig = df_signal['InvMass'].plot.hist(bins=100, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invariant-mass distribution of the background (like-sign)\n",
    "minv_bkg = df_background['InvMass'].plot.hist(bins=100, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#invariant-mass distribution of real data\n",
    "minv_data = df_data['InvMass'].plot.hist(bins=100, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"trick\" to plot more than one distribution is to create a new dataframe with the data to plot in different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe with invariant-mass of background and signal\n",
    "df_new = pd.concat([df_background['InvMass'], df_signal['InvMass']], axis=1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minv_compared = df_new.plot.hist(bins=100, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "We need to tell at the model what is signal and what is background.\n",
    "\n",
    "So we add a 'y' column and label signal and background with **y=1** for signal and **y=0** for background. Then we stack togheter signal and background. This will be the reference for the ML model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signal['y'] = 1\n",
    "df_background['y'] = 0\n",
    "\n",
    "df_ml = pd.concat([df_signal, df_background], axis=0)\n",
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring training variables (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Exercise:** looking at what we did to plot two distributions togheter compare the 'HypCandPt' disribution between  signal and background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au.show_solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex = pd.concat([df_background['HypCandPt'], df_signal['HypCandPt']], axis=1)\n",
    "pt_plot = df_ex.plot.hist(bins=100, alpha=0.6, density=True)\n",
    "pt_plot.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this for all the variables we have implemented and utility function `plot_distr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define the variables to plot\n",
    "columns = ['HypCandPt',\n",
    "           'TPCnSigmaHe3',\n",
    "           'V0CosPA',\n",
    "           'ProngsDCA',\n",
    "           'He3ProngPvDCA',\n",
    "           'PiProngPvDCA',\n",
    "           'He3ProngPvDCAXY',\n",
    "           'PiProngPvDCAXY',\n",
    "           'NpidClustersHe3']\n",
    "\n",
    "au.plot_distr(df_ml, columns, bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to look also at the correlations between the variables. The model can potentially exploit them to perform a better classification. Moreover, there could be some potentially dangerous correlatios as those with the invariant mass of the particle of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au.plot_corr(df_ml, columns, 'signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au.plot_corr(df_ml, columns, 'background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Train and Test set\n",
    "Now we split our data in a training and test set. To do it, we use the `train_test_split` function from the library sklearn <https://scikit-learn.org/stable/>. \n",
    "\n",
    "To access the function documentation use Shift+Tab after the first parenthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, y_train, y_test = sklearn.model_selection.train_test_split(df_ml[columns], df_ml['y'], \n",
    "                                                                                test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test the lenght of train and test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and application\n",
    "### Example with simple (and weak) model\n",
    "\n",
    "Let's start with a simple model like the naive bayes <https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html>.\n",
    "\n",
    "* Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "#initialize a the model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "#train the model\n",
    "nb_model.fit(train_set[columns], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply the model on the test set and evaluate its performace using the ROC curve <https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the model on the test set\n",
    "y_nb = nb_model.predict(test_set[columns])\n",
    "\n",
    "#evaluate the model performance\n",
    "nb_score = sklearn.metrics.roc_auc_score(y_test, y_nb)\n",
    "print(f'ROC_AUC_score: {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now apply the model on the real data that we want to classify and look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_model.predict(df_data[columns])\n",
    "\n",
    "#print some of the predictions\n",
    "print(y_pred[10:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the predictions to the data dataframe\n",
    "df_data['y_nb'] = y_pred\n",
    "\n",
    "#invariant-mass distribution after the selections\n",
    "df_data.query('y_nb>0.5')['InvMass'].plot.hist(bins=100, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better model: XGBoost\n",
    "\n",
    "XGBoost is a quite popular library for Boosted Decision Trees <https://xgboost.readthedocs.io/en/latest/>. It is more complex than the previous model and has many parameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common configuration parameters\n",
    "xgb_params = {'objective': 'binary:logistic', \n",
    "              'tree_method': 'hist',\n",
    "              'n_jobs': 2,\n",
    "              'random_state': 42,\n",
    "              'silent': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model\n",
    "We can start with a configuration on hyper-parameters that gives a **simple model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {'max_depth': 2,\n",
    "               'learning_rate': 0.01,\n",
    "               'n_estimators': 25,\n",
    "               'subsample': 0.7,\n",
    "               'colsample_bytree': 0.6}\n",
    "params = {**xgb_params, **hyperparams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize a the model\n",
    "simple_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "#train the model\n",
    "_ = simple_model.fit(train_set[columns], y_train)\n",
    "\n",
    "#apply the model on the test set\n",
    "y_test_pred = simple_model.predict_proba(test_set[columns]).T[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the model predictions on signal and background between the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "au.plot_output_train_test(simple_model, train_set, y_train, test_set, y_test, columns=columns,\n",
    "                          figsize=(7,6), log=True, location='upper center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the roc curve\n",
    "au.plot_roc(y_test, y_test_pred)\n",
    "\n",
    "#evaluate the model performance\n",
    "model_score = sklearn.metrics.roc_auc_score(y_test, y_test_pred)\n",
    "print(f'ROC_AUC_score: {model_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to know which variables are more important in the classification using the feature importance given by the XGBoost library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "au.plot_feature_imp(simple_model, imp_list=['gain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model application on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the model on the real data\n",
    "y_pred = simple_model.predict(df_data[columns], output_margin=True)\n",
    "df_data.eval('score_simple = @y_pred', inplace=True)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the final classification we still need to choose a threshold value on the BDT score. Particle candidates with a BDT score above this threshold will be considered signal and selected.\n",
    "\n",
    "To select the threshold we can look at the efficiency vs threshold for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BDT efficiency\n",
    "def eff_scan(model, test_set, columns):\n",
    "    y_pred = model.predict(test_set[columns], output_margin=True)\n",
    "\n",
    "    test_set.eval('score = @y_pred', inplace=True)\n",
    "    test_set.eval('y = @y_test', inplace=True)\n",
    "    min_score = test_set['score'].min()\n",
    "    max_score = test_set['score'].max()\n",
    "    threshold = np.linspace(min_score, max_score, 100)\n",
    "    efficiency = []\n",
    "    n_sig = sum(test_set['y'])\n",
    "\n",
    "    for t in threshold:\n",
    "        df_selected = test_set.query('score>@t')['y']\n",
    "        sig_selected = np.sum(df_selected)\n",
    "        efficiency.append(sig_selected / n_sig)\n",
    "\n",
    "    au.plot_bdt_eff(threshold, efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the scan\n",
    "eff_scan(simple_model, test_set, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a threshold value\n",
    "thr = 0.\n",
    "\n",
    "#invariant-mass distribution after the selections\n",
    "inv_sel_simple = df_data.query(f'score_simple > {thr}')['InvMass']\n",
    "_ = inv_sel_simple.plot.hist(bins=100, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the distribution and extract the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy import integrate\n",
    "\n",
    "def fit_invmass(df):\n",
    "    \n",
    "    # histogram of the data\n",
    "    counts, bins = np.histogram(df, bins=40, range=[2.96, 3.05])\n",
    "    \n",
    "    # define functions for fitting    \n",
    "    def gaus_function(x, N, mu, sigma):\n",
    "        return N * np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "    \n",
    "    def pol2_function(x, a, b):\n",
    "        return (a + x*b)\n",
    "    \n",
    "    def fit_function(x, a, b, N, mu, sigma):\n",
    "        return pol2_function(x, a, b) + gaus_function(x, N, mu, sigma)\n",
    "    \n",
    "    # x axis ranges for plots\n",
    "    x_point = 0.5 * (bins[1:] + bins[:-1])\n",
    "    r = np.arange(2.96, 3.05, 0.00001)\n",
    "    r_red = np.arange(2.98, 3.005, 0.0001)\n",
    "    \n",
    "    # fit the invariant mass distribution with fit_function() pol2+gauss\n",
    "    popt, pcov = curve_fit(fit_function, x_point, counts, p0 = [100, -1, 100, 2.99, 0.001])\n",
    "    \n",
    "    # plot data\n",
    "    plt.errorbar(x_point, counts, yerr=np.sqrt(counts), fmt='.', ecolor='k', color='k', elinewidth=1., label='Data')\n",
    "    \n",
    "    # plot pol2 and gauss obtained in the fit separately\n",
    "    plt.plot(r_red, gaus_function(r_red, N=popt[2], mu=popt[3], sigma=popt[4]), label='gaus', color='red')\n",
    "    plt.plot(r, pol2_function(r, a=popt[0], b=popt[1]), label='pol2', color='green')\n",
    "\n",
    "    # plot the global fit\n",
    "    plt.plot(r, fit_function(r, *popt), label='pol2+gauss', color='blue')\n",
    "    \n",
    "    # compute significance of the signal\n",
    "    signal = integrate.quad(gaus_function, 2.98, 3.005, args=(popt[2], popt[3], popt[4]))[0] / 0.00225\n",
    "    background = integrate.quad(pol2_function, 2.98, 3.005, args=(popt[0], popt[1]))[0] / 0.00225\n",
    "    print(f'Signal counts: {signal:.0f}')\n",
    "    print(f'Background counts: {background:.0f}')     \n",
    "    significance = signal / np.sqrt(signal + background)\n",
    "\n",
    "    # Add some axis labels\n",
    "    plt.title(f'significance: {significance:.1f}')\n",
    "    plt.legend()\n",
    "    plt.xlabel('$M_{^{3}He+\\pi}$ $(\\mathrm{GeV/}c^2)$')\n",
    "    plt.ylabel('counts / 2.25 $\\mathrm{MeV/}c^2$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_invmass(inv_sel_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized model\n",
    "We can optimize the hyper-parameters to have a **more complex model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {'max_depth': 13,\n",
    "               'learning_rate': 0.0982,\n",
    "               'n_estimators': 181,\n",
    "               'gamma': 0.4467,\n",
    "               'min_child_weight': 5.75,\n",
    "               'subsample': 0.74,\n",
    "               'colsample_bytree': 0.57}\n",
    "params = {**xgb_params, **hyperparams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a the model\n",
    "opt_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "#train the model\n",
    "opt_model.fit(train_set[columns], y_train)\n",
    "\n",
    "#apply the model on the test set\n",
    "y_test_pred = opt_model.predict_proba(test_set[columns]).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model performance\n",
    "model_score = sklearn.metrics.roc_auc_score(y_test, y_test_pred)\n",
    "print(f'ROC_AUC_score: {model_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the predictions on train and test set\n",
    "au.plot_output_train_test(opt_model, train_set, y_train, test_set, y_test, columns=columns,\n",
    "                          figsize=(7,6), log=True, location='upper center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the model on the real data\n",
    "y_pred = opt_model.predict(df_data[columns], output_margin=True)\n",
    "df_data.eval('score_opt = @y_pred', inplace=True)\n",
    "\n",
    "#perform the scan\n",
    "eff_scan(opt_model, test_set, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a threshold value\n",
    "thr_opt = 3.24\n",
    "\n",
    "#invariant-mass distribution after the selections\n",
    "inv_sel_opt = df_data.query(f'score_opt > {thr_opt}')['InvMass']\n",
    "df_comp = pd.concat([inv_sel_simple, inv_sel_opt], axis=1)\n",
    "mass_plot = df_comp.plot.hist(bins=100, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the distribution\n",
    "fit_invmass(inv_sel_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass Shaping\n",
    "\n",
    "Let's see what could happens if the model overfit the data. We can train a too complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "columns = ['V0CosPA',\n",
    "           'HypCandPt',\n",
    "           'ProngsDCA',\n",
    "           'PiProngPvDCAXY',\n",
    "           'He3ProngPvDCAXY',\n",
    "           'He3ProngPvDCA',\n",
    "           'PiProngPvDCA',\n",
    "           'NpidClustersHe3',\n",
    "           'TPCnSigmaHe3',\n",
    "           'PiProngPt',\n",
    "           'He3ProngPt']\n",
    "\n",
    "#the model training is very long, we have already trained it\n",
    "complex_model = pickle.load(open('/eos/user/a/alicesk/sk2019/data/complex_model.sav', 'rb'))\n",
    "\n",
    "y_pred = complex_model.predict(df_data[columns], output_margin=True)\n",
    "df_data.eval('score = @y_pred', inplace=True)\n",
    "\n",
    "y_pred = complex_model.predict(df_background[columns], output_margin=True)\n",
    "df_background.eval('score = @y_pred', inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the model application on the data is quite slow and we already performed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with score given by overtrained model\n",
    "df_data = pd.read_pickle('/eos/user/a/alicesk/sk2019/data/data_with_score.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.query('score>4.5')['InvMass'].plot.hist(bins=40, alpha=0.6, xlim=(2.96,3.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_data.query('score>4.5')['InvMass']\n",
    "fit_invmass(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load background data with score given by overtrained model\n",
    "df_background = pd.read_pickle('data/background_with_score.pkl')\n",
    "inv_mass_bkg = df_background.query('score>4.5')['InvMass']\n",
    "inv_mass_bkg.plot.hist(bins=40, alpha=0.6, xlim=(2.96,3.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Excercise\n",
    "\n",
    "Using the query function of pandas perform the \"standard\" selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lin_sel = df_data.query('V0CosPA > 0.995')\n",
    "inv_mass_lin_sel = df_lin_sel['InvMass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_invmass(inv_mass_lin_sel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
